---
title: "Microfracture_Code+Figs"
author: "Issac Sujay Anand John Jayachandran"
date: "2023-02-05"
output: html_document
---

```{r data split and normalization, echo=FALSE}

# Loading the requisite libraries
library(tidyverse)
library(dplyr)
library(caret)
library(magrittr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(MASS)

# Insert the path for the complete tabular dataset
complete_df <- read.csv("insert path of the csv here")

# Extracting the labelled data from the complete dataset
label_df <- complete_df %>% 
  filter(Type != "Unlabelled") %>% dplyr::select(c(3:15))

# rearranging the columns to group like features
label_df <- label_df %>% dplyr::select(c(1:11, 13, 12))

# splitting the labelled df into training and testing 
set.seed(2504) # setting a definite seed value to ensure repeatability
label_partition <- createDataPartition(label_df$Type, p = 0.7, list = F)
train_df <- label_df[label_partition, ]
test_df <- label_df[-label_partition, ]

# Training data pre-processing

# Transforming the skewed variables prior to centering and scaling
train_df_transformed <- train_df %>% 
  mutate_at(vars(c(7, 9, 11)), log) # logging ff, AR, and roundness

# Normalizing the training and testing df separately
train_df_NT <- train_df_transformed %>%  mutate_at(vars(c(5:11)), ~(scale(.)))

# Extracting the statistics of the transformed and normalized training data
train_stats <- train_df_transformed %>% 
  dplyr::select(c(5:11)) %>%
  summarise_all(list(mean = mean, sd = sd))

# Testing data pre-processing
test_df_transformed <- test_df %>% 
  mutate_at(vars(c(7, 9, 11)), log) 

# Normalizing the testing data using the standard deviation and mean from the training data
test_df_NT <- test_df_transformed %>%  
  mutate(solidity = scale(solidity, center = train_stats$solidity_mean, scale = train_stats$solidity_sd),
         extent = scale(extent, center = train_stats$extent_mean, scale = train_stats$extent_sd),
         formfactor = scale(formfactor, center = train_stats$formfactor_mean, scale = train_stats$formfactor_sd),
         compactness = scale(compactness, center = train_stats$compactness_mean, scale = train_stats$compactness_sd),
         aspect_ratio = scale(aspect_ratio, center = train_stats$aspect_ratio_mean, scale = train_stats$aspect_ratio_sd),
         circularity = scale(circularity, center = train_stats$circularity_mean, scale = train_stats$circularity_sd),
         roundness = scale(roundness, center = train_stats$roundness_mean, scale = train_stats$roundness_sd))


# Label df normalization
label_df_NT <- label_df %>% 
  mutate_at(vars(c(7, 9, 11)), log) %>%
  mutate_at(vars(c(5:11)), ~(scale(.)))

```

```{r bar plots of secondary labels}
# Bar plot of the secondary labels from the training and testing data
train_df_label <- train_df %>% 
  mutate(data_type = "train") %>% 
  group_by(Pore.type, data_type) %>% 
  summarize(count = n()) %>%
  mutate(prop = count/560)
  
  
test_df_label <- test_df %>% mutate(data_type = "test") %>%
  group_by(Pore.type, data_type) %>% 
  summarize(count = n()) %>%
  mutate(prop = count/240)

combined_df_label <- rbind(train_df_label, test_df_label)
  

sec_label_barplot <- combined_df_label %>% 
  ggplot(aes(fill = data_type, x = Pore.type, y = prop)) +
  geom_bar(position = "dodge", stat = "identity") + 
  ylab("Proportion") +
  xlab("Pore type") + 
  scale_fill_manual(values = c("#DE0018", "#2D69A9"), 
                      breaks = c("train", "test"), 
                      labels = c("Train", "Test")) +
  theme_clean()

sec_label_barplot

```

```{r Fig___ Histograms of all shape features from whole dataset, echo=FALSE}
# solidity histogram
w1 <- ggplot(merged_df_prenormalized, aes(x = solidity)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  theme_clean()

# compactness histogram
w2 <- ggplot(merged_df_prenormalized, aes(x = compactness)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  theme_clean()


# formfactor histogram
w3 <- ggplot(merged_df_prenormalized, aes(x = formfactor)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  theme_clean()



# extent histogram
w4 <- ggplot(merged_df_prenormalized, aes(x = extent)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  theme_clean()



# circularity histogram
w5 <- ggplot(merged_df_prenormalized, aes(x = circularity)) + 
  geom_histogram(binwidth = 0.02, color = "black", fill = "white") +
  theme_clean()



# roundness histogram
w6 <- ggplot(merged_df_prenormalized, aes(x = roundness)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  theme_clean()


# AR histogram
w7 <- ggplot(merged_df_prenormalized, aes(x = aspect_ratio)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white", alpha = 0.5) + theme_clean()


#grid.arrange(w1, w2, w3, w4, w5, w6, w7, ncol = 3)


```


```{r Fig___ Histograms of all shape features from training df, echo=FALSE}
# solidity histogram
p1 <- ggplot(train_df, aes(x = solidity)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  geom_histogram(data = test_df, aes(x = solidity), binwidth = 0.05, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# compactness histogram
p2 <- ggplot(train_df, aes(x = compactness)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  geom_histogram(data = test_df, aes(x = compactness), binwidth = 0.05, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# formfactor histogram
p3 <- ggplot(train_df, aes(x = formfactor)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  geom_histogram(data = test_df, aes(x = formfactor), binwidth = 0.05, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# extent histogram
p4 <- ggplot(train_df, aes(x = extent)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  geom_histogram(data = test_df, aes(x = extent), binwidth = 0.05, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# circularity histogram
p5 <- ggplot(train_df, aes(x = circularity)) + 
  geom_histogram(binwidth = 0.02, color = "black", fill = "white") +
  geom_histogram(data = test_df, aes(x = circularity), binwidth = 0.02, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# roundness histogram
p6 <- ggplot(train_df, aes(x = roundness)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white") +
  geom_histogram(data = test_df, aes(x = roundness), binwidth = 0.05, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# AR histogram
p7 <- ggplot(train_df, aes(x = aspect_ratio)) + 
  geom_histogram(binwidth = 0.05, color = "black", fill = "white", alpha = 0.5) +
  geom_histogram(data = test_df, aes(x = aspect_ratio), binwidth = 0.05, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


grid.arrange(p1, p2, p3, p4, p5, p6, p7, ncol = 3)

```


```{r Transformed distributions, echo=FALSE}

# converting the primary and secondary labels to factors
## Training dataset
train_df_NT <- train_df_NT %>% 
  mutate_at(vars(Type, Pore.type), as.factor)

## Testing dataset
test_df_NT <- test_df_NT %>% 
  mutate_at(vars(Type, Pore.type), as.factor)

# solidity histogram
t1 <- ggplot(train_df_NT, aes(x = solidity)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white") +
  geom_histogram(data = test_df_NT, aes(x = solidity), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()

# compactness histogram
t2 <- ggplot(train_df_NT, aes(x = compactness)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white") +
  geom_histogram(data = test_df_NT, aes(x = compactness), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()

# formfactor histogram
t3 <- ggplot(train_df_transformed, aes(x = formfactor)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white") +
  geom_histogram(data = test_df_transformed, aes(x = formfactor), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# extent histogram
t4 <- ggplot(train_df_NT, aes(x = extent)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white") +
  geom_histogram(data = test_df_NT, aes(x = extent), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()

# circularity histogram
t5 <- ggplot(train_df_NT, aes(x = circularity)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white") +
  geom_histogram(data = test_df_NT, aes(x = circularity), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()

# roundness histogram
t6 <- ggplot(train_df_transformed, aes(x = roundness)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white") +
  geom_histogram(data = test_df_transformed, aes(x = roundness), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()


# AR histogram
t7 <- ggplot(train_df_transformed, aes(x = aspect_ratio)) + 
  geom_histogram(binwidth = 0.2, color = "black", fill = "white", alpha = 0.5) +
  geom_histogram(data = test_df_transformed, aes(x = aspect_ratio), binwidth = 0.2, color = "black", fill = "skyblue", alpha = 0.5) +
  theme_clean()

final_label_hist <- grid.arrange(t1, t2, t3, t4, t5, t6, t7, ncol = 3)




```

```{r Boxplots of labelled data per shape, echo=FALSE}
library(reshape2)
label_boxplot <- melt(train_df_NT,id.vars='Type', measure.vars = 5:9)
sec_label_boxplot <- ggplot(label_boxplot) +
  geom_boxplot(aes(x=variable, y=value, color=Type)) +
  xlab("Shape feature") +
  scale_colour_manual(values = c("#DE0018", "#2D69A9"), 
                      breaks = c("Crack", "Pore"), 
                      labels = c("Microfracture", "Pore")) +
  theme_clean()

```



```{r Fig 3 Correlation heatmap on trained normalized data, echo=FALSE}

library(ggcorrplot)

cor_mat <- train_df_NT %>%
  dplyr::select((c(5:11))) %>% 
  cor

cor_plot <- ggcorrplot(cor_mat, 
                       method = "square", 
                       hc.order = T, 
                       type = "lower",
                       colors = c("#2D69A9", "white", "#DE0018"),
                       outline.color = "white",
                       ggtheme = ggplot2::theme_classic(),
                       lab = T,
                       tl.srt = 90,
                       digits = 2,
                       show.diag = T,
                       legend.title = "Correlation")

cor_plot


```


No need to include graphics in this markdown as I will have to do it in overleaf anyways.
```{r Fig 4 shape diagrams, echo=FALSE, fig.cap="Visual representations of the final shape features selected. Adapted from Ghiasi-Freez et al. (2012)."}
#knitr::include_graphics("")
```


```{r Fig __ secondary labels of pores and microfractures, echo=FALSE}
#knitr::include_graphics("")
```

I can split this into four facets; scree plot, overall biplot with the labelled datapoints, the PC1 and PC2 loadings per feature. For this I can use the entire dataset without splitting into training and testing as it is simply to provide a baseline value.

```{r transforming the merged dataset}

# organizing the merged df to be identical to training and testing df
merged_df_clean <- merged_df_prenormalized %>% 
  dplyr::select(-c(1:2))

# Normalizing the merged df
merged_df_NT <-  
  merged_df_clean %>%
  mutate_at(vars(c(7,9)), log) %>%
  mutate_at(vars(c(5:11)), ~(scale(.)))

merged_df_NT <- merged_df_NT %>% 
  mutate_at(vars(Type, Pore.type), as.factor)

```

```{r PCA plots for merged df}
library(ggfortify)
library(ggplot2)
library("FactoMineR")
library("factoextra")
library(stats)

set.seed(2504)
merged_pca <- merged_df_NT %>% 
  dplyr::select(c(5:9)) %>%
  prcomp()

merged_scores <- as.data.frame(merged_pca$x)
merged_loadings <- as.data.frame(merged_pca$rotation)
colnames(merged_scores) <- paste0("PC", 1:ncol(merged_scores))
colnames(merged_scores) <- paste0("PC", 1:ncol(merged_loadings))

classes <- merged_df_NT$Type

merged_scores <- data.frame(merged_scores, classes)
merged_scores$classes <- factor(merged_scores$classes, levels = c("Unlabelled", "Crack", "Pore"))

merged_scores_sorted <- merged_scores[order(merged_scores$classes), ]

label_loadings <- as.data.frame(merged_pca$rotation)
label_loadings$variable <- rownames(label_loadings)
label_loadings <- gather(label_loadings, "principal_component", "loading_score", -variable)


# Loading plots
ggplot(label_loadings, aes(x = variable, y = PC2)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Variable", y = "Loading Score") + theme_few()

# Plot biplot
## Unlabelled whole dataset PCA biplot
library(ggfortify)
whole_biplot_unlabel <- autoplot(merged_pca,
                              data = merged_df_NT, 
                              fill = "yellow",
                              colour = "black",
                              shape = 21,
                              size = 2,
                              loadings = TRUE,
                              loadings.colour = 'black',
                              loadings.label = TRUE, 
                              loadings.label.size = 5,
                              loadings.label.colour = "black") +
  theme_few()

## Whole dataset PCA biplot with the labelled datapoints highlighted
whole_biplot_label <- autoplot(merged_pca, 
                              data = merged_scores_sorted, 
                              colour = 'classes',
                              size = 2.4,
                              alpha = 0.7,
                              loadings = TRUE,
                              loadings.colour = 'black',
                              loadings.label = TRUE, 
                              loadings.label.size = 3,
                              loadings.label.colour = "black") +
  #geom_point(size=1.5, alpha=0.8, colour="black", pch=21) + 
  scale_colour_manual(values = c("grey90","#DE0018", "#2D69A9"), 
                      breaks = c("Unlabelled", "Crack", "Pore"), 
                      labels = c("Unlabelled", "Microfracture", "Pore")) +
  theme_few()



```

```{r Fig __ PCA plots, echo=FALSE}
# train_pca is the prcomp object which contains the scores of each point and the loadings of the features
set.seed(2504)
train_pca <- train_df_NT %>% 
  dplyr::select(c(5:9)) %>%
  prcomp()

train_scores <- as.data.frame(train_pca$x)
train_loadings <- as.data.frame(train_pca$rotation)
colnames(train_scores) <- paste0("PC", 1:ncol(train_scores))
colnames(train_scores) <- paste0("PC", 1:ncol(train_loadings))

# Using the overall labelled df instead of only the training
set.seed(2504)
label_pca <- label_df_NT %>% 
  dplyr::select(c(5:9)) %>%
  prcomp()

label_scores <- as.data.frame(label_pca$x)
colnames(label_scores) <- paste0("PC", 1:ncol(label_scores))
colnames(label_scores) <- paste0("PC", 1:ncol(label_loadings))

# prepping the loadings data for the loadings plot
label_loadings <- as.data.frame(label_pca$rotation)
label_loadings$variable <- rownames(label_loadings)
label_loadings <- gather(label_loadings, "principal_component", "loading_score", -variable)


# Loading plots
#ggplot(label_loadings, aes(x = variable, y = PC2)) +
 # geom_bar(stat = "identity", position = "dodge") +
  #labs(x = "Variable", y = "Loading Score") + theme_few()



# Plot biplot
prim_label_biplot <- autoplot(label_pca, 
                              data = label_df_NT, 
                              colour = 'Type',
                              size = 2.4,
                              alpha = 0.6,
                              loadings = TRUE,
                              loadings.colour = 'black',
                              loadings.label = TRUE, 
                              loadings.label.size = 3,
                              loadings.label.colour = "black") +
  #geom_point(size=2.5, alpha=0.9, colour="black", pch=21) + 
  scale_colour_manual(values = c("#DE0018", "#2D69A9"), 
                      breaks = c("Crack", "Pore"), 
                      labels = c("Microfracture", "Pore")) +
  theme_few()


sec_label_biplot <- autoplot(label_pca, 
                             data = label_df_NT, 
                             colour = 'Pore.type',
                             shape = 'Type',
                             size = 2.4,
                             loadings = TRUE,
                             loadings.colour = 'black',
                             loadings.label = TRUE, 
                             loadings.label.size = 3,
                             loadings.label.colour = "black") +
  scale_colour_manual(values = c("#2D69A9","#ff5000", "#7804c8", "#1f6932",
                                 "#571f4e", "#a2faa3", "#4f759b", "#92c9b1","#5d5179"), 
                      breaks = c("Straight", "Curvilinear", "Curved", "Branching",
                                 "Vug", "Intercrystalline", "Intraparticle", "Channel",
                                 "Hybrid"), 
                      labels = c("Straight", "Curvilinear", "Curved", "Branching",
                                 "Vug", "Intercrystalline", "Intraparticle", "Channel",
                                 "Hybrid")) +
  theme_few()

pca_grid <- grid.arrange(whole_biplot_unlabel, 
             whole_biplot_label,
             prim_label_biplot,
             sec_label_biplot,
             ncol = 2)



```


```{r PCA for microfractures subclasses, echo=FALSE}
library(GGally)

# Microfracture sub-classes

## Separating the raw microfracture labelled objects
cracks_df <- label_df %>% filter(Type == "Crack")

## Converting type and pore type to factors
cracks_df <- cracks_df %>% 
  mutate_at(vars(Type, Pore.type), as.factor) %>% filter(Pore.type != "Hybrid")

cracks_pairplot <- ggpairs(cracks_df, columns = 5:9, ggplot2::aes(colour=Pore.type))

cracks_pairplot

cracks_df_normalized <- cracks_df %>%  mutate_at(vars(c(5:9)), ~(scale(.)))

## PCA
set.seed(2504)
cracks_pca <- cracks_df_normalized %>% 
  dplyr::select(c(5:9)) %>%
  prcomp()

cracks_scores <- as.data.frame(cracks_pca$x)
cracks_loadings <- as.data.frame(cracks_pca$rotation)
colnames(cracks_scores) <- paste0("PC", 1:ncol(cracks_scores))
colnames(cracks_scores) <- paste0("PC", 1:ncol(cracks_loadings))

cracks_biplot <- autoplot(cracks_pca, 
                             data = cracks_df_normalized, 
                             colour = 'Pore.type',
                             size = 2.4,
                             loadings = TRUE,
                             loadings.colour = 'black',
                             loadings.label = TRUE, 
                             loadings.label.size = 3,
                             loadings.label.colour = "black") +
  scale_colour_manual(values = c("#2D69A9","#ff5000", "#7804c8", "#1f6932"), 
                      breaks = c("Straight", "Curvilinear", "Curved", "Branching"), 
                      labels = c("Straight", "Curvilinear", "Curved", "Branching")) +
  theme_few()

cracks_biplot

# Pore sub-classes
pores_df <- label_df %>% filter(Type == "Pore")

## Converting type and pore type to factors
pores_df <- pores_df %>% 
  mutate_at(vars(Type, Pore.type), as.factor) %>% filter(Pore.type != "Intraparticle")

pores_pairplot <- ggpairs(pores_df, columns = 5:9, ggplot2::aes(colour=Pore.type))

pores_pairplot

pores_df_normalized <- pores_df %>%  mutate_at(vars(c(5:9)), ~(scale(.)))

## PCA
set.seed(2504)
pores_pca <- pores_df_normalized %>% 
  dplyr::select(c(5:9)) %>%
  prcomp()

pores_scores <- as.data.frame(pores_pca$x)
pores_loadings <- as.data.frame(pores_pca$rotation)
colnames(pores_scores) <- paste0("PC", 1:ncol(pores_scores))
colnames(pores_scores) <- paste0("PC", 1:ncol(pores_loadings))

pores_biplot <- autoplot(pores_pca, 
                             data = pores_df_normalized, 
                             shape = 'Pore.type', colour = 'Pore.type',
                             size = 2.4,
                             loadings = TRUE,
                             loadings.colour = 'black',
                             loadings.label = TRUE, 
                             loadings.label.size = 3,
                             loadings.label.colour = "black") +
  scale_colour_manual(values = c("#2D69A9","#ff5000", "#7804c8", "#1f6932"), 
                      breaks = c("Channel", "Intercrystalline", "Vug"), 
                      labels = c("Channel", "Intercrystalline", "Vug")) +
  theme_few()

pores_biplot

```


```{r ML pipeline, echo=FALSE}
# Defining the HPO CV parameters
cvspecs_linear <- trainControl(method = "repeatedcv",
                        number = 10,
                        repeats = 10,
                        savePredictions = "all",
                        classProbs = T)


cvspecs_grid <- trainControl(method = "repeatedcv",
                             number = 10,
                             repeats = 10,
                             savePredictions = "all",
                             classProbs = T,
                             search = "grid")

# Linear models

## MLR
set.seed(2504)
mlr_model <- train(Type ~ solidity + extent + formfactor + compactness +
                     aspect_ratio, 
                   data = train_df_NT, 
                   method = "glm", 
                   family = binomial,
                   trControl = cvspecs_linear)

mlr_model
summary(mlr_model)

### Applying model on testing data
mlr_model_class <- predict(mlr_model, test_df_NT, type = "raw") #this gives the predicted classes as a list
mlr_model_prob <- predict(mlr_model, test_df_NT, type = "prob") #this gives the probabilities for each selection

#Confusion matrix
conmat_mlr_model <- confusionMatrix(data = mlr_model_class, reference = test_df_NT$Type)
conmat_mlr_model


### Learning  (Always on the training data)
set.seed(2504)
mlr_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "glm", 
                               family = binomial,
                               metric = "Accuracy",
                               trControl = cvspecs_linear)

mlr_learnplot <- ggplot(mlr_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("MLR")

## LDA
set.seed(2504)
lda_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                    data = train_df_NT, 
                    method = "lda",
                    trControl = cvspecs_linear)

lda_model
summary(lda_model)

### Applying model on testing data
lda_model_class <- predict(lda_model, test_df_NT, type = "raw") #this gives the predicted classes as a list
lda_model_prob <- predict(lda_model, test_df_NT, type = "prob") #this gives the probabilities for each selection

### Confusion matrix
conmat_lda_model <- confusionMatrix(data = lda_model_class, reference = test_df_NT$Type)
conmat_lda_model

### Learning  (Always on the training data)
set.seed(2504)
lda_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "lda", 
                               metric = "Accuracy",
                               trControl = cvspecs_linear)

lda_learnplot <- ggplot(lda_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("LDA")

## QDA
set.seed(2504)
qda_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                    data = train_df_NT, 
                    method = "qda",
                    trControl = cvspecs_linear)

qda_model
summary(qda_model)

### Applying model on testing data
qda_model_class <- predict(qda_model, test_df_NT, type = "raw") #this gives the predicted classes as a list
qda_model_prob <- predict(qda_model, test_df_NT, type = "prob") #this gives the probabilities for each selection

#Confusion matrix
conmat_qda_model <- confusionMatrix(data = qda_model_class, reference = test_df_NT$Type)
conmat_qda_model

### Learning curve
set.seed(2504)
qda_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "qda", 
                               metric = "Accuracy",
                               trControl = cvspecs_linear)

qda_learnplot <- ggplot(qda_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("QDA")



# Non-linear models

## kNN
library(class)
set.seed(2504)
knn_tunegrid <- data.frame(k = seq(1, 51, by = 2))

set.seed(2504)
knn_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                   data = train_df_NT, 
                   method = "knn",
                   trControl = cvspecs_grid,
                   tuneGrid = knn_tunegrid)

knn_model
summary(knn_model)

### HPO plot
knn_hpo_plot <- ggplot(knn_model, aes(x = k, y = Accuracy)) +
  geom_line() +
  geom_point(fill = "white", colour = "black", size = 2) +
  labs(x = "Number of nearest neighbors (k)", y = "Accuracy") +
  ggtitle("Hyperparameter Optimization for KNN Model") + 
  theme_few()


### Applying model on testing data
knn_model_class <- predict(knn_model, test_df_NT, type = "raw") #this gives the predicted classes as a list
knn_model_prob <- predict(knn_model, test_df_NT, type = "prob") #this gives the probabilities for each selection

#Confusion matrix
conmat_knn_model <- confusionMatrix(data = knn_model_class, reference = test_df_NT$Type)
conmat_knn_model

### Learning curve
set.seed(2504)
knn_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "knn", 
                               metric = "Accuracy",
                               trControl = cvspecs_grid)
knn_learnplot <- ggplot(knn_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("kNN")


## NB
library("klaR")
set.seed(2504)
nb_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                   data = train_df_NT, method = "nb",
                   trControl = cvspecs_grid,
                   tuneGrid = data.frame(fL=seq(1, 10, length = 10), usekernel = TRUE, adjust=seq(1, 10, length = 10)))

nb_model

### HPO plot
nb_hpo_plot <- ggplot(nb_model, aes(x = fl, y = Accuracy)) +
  geom_line() +
  geom_point(fill = "white", colour = "black", size = 2) +
  labs(x = "fl", y = "Accuracy") +
  ggtitle("Hyperparameter Optimization for Naive-Bayes Model") + 
  theme_few()

### learning curve
set.seed(2504)
nb_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "nb", 
                               metric = "Accuracy",
                               trControl = cvspecs_grid,
                               )

nb_learnplot <- ggplot(nb_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("NB")

### Prediction on test data
nb_model_class <- predict(nb_model, test_df_NT)
nb_model_prob <- predict(nb_model, test_df_NT, type = "prob")


conmat_nb_model <- confusionMatrix(data = nb_model_class, test_df_NT$Type)
conmat_nb_model


## RF
library(ranger)
rf_tuneGrid <- expand.grid(mtry = c(2:5))

set.seed(2504)
rf_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                  data = train_df_NT, 
                  method = "rf",
                  trControl = cvspecs_grid,
                  tuneGrid = rf_tuneGrid)
rf_model

### HPO plot
rf_hpo_plot <- ggplot(rf_model, aes(x = mtry, y = Accuracy)) +
  geom_line() +
  geom_point(fill = "white", colour = "black", size = 2) +
  labs(x = "mtry", y = "Accuracy") +
  ggtitle("Hyperparameter Optimization for Random Forest Model") + 
  theme_few()


### Predicting on the test set
rf_model_class <- predict(rf_model, test_df_NT)
rf_model_prob <- predict(rf_model, test_df_NT, type = "prob")

conmat_rf_model <- confusionMatrix(data = rf_model_class, test_df_NT$Type)
conmat_rf_model

### learning curve
set.seed(2504)
rf_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "rf", 
                               metric = "Accuracy",
                               trControl = cvspecs_grid,
                               )

rf_learnplot <- ggplot(rf_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("RF")


## SVM Linear
linsvm_tunegrid <- data.frame(C = seq(0.01, 1, by = 0.1))

set.seed(2504)
linsvm_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                    data = train_df_NT, 
                    method = "svmLinear",
                    trControl = cvspecs_grid,
                    tuneGrid = linsvm_tunegrid)

linsvm_model

### HPO plot
linsvm_hpo_plot <- ggplot(linsvm_model, aes(x = C, y = Accuracy)) +
  geom_line() +
  geom_point(fill = "white", colour = "black", size = 2) +
  labs(x = "Cost", y = "Accuracy") +
  ggtitle("Hyperparameter Optimization for Linear SVM Model") + 
  theme_few()

linsvm_model_class <- predict(linsvm_model, test_df_NT)
linsvm_model_prob <- predict(linsvm_model, test_df_NT, type = "prob")

conmat_linsvm_model <- confusionMatrix(data = linsvm_model_class, test_df_NT$Type)
conmat_linsvm_model

### learning curve
set.seed(2504)
linsvm_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "svmLinear", 
                               metric = "Accuracy",
                               trControl = cvspecs_grid,
                               )

linsvm_learnplot <- ggplot(linsvm_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few()+ ggtitle("SVM Linear")


## SVM radial
set.seed(2504)
radsvm_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                    data = train_df_NT, 
                    method = "svmRadial",
                    trControl = cvspecs_grid,
                    tuneGrid = expand.grid(C = seq(1,20, length = 10),
                                           sigma = seq(0.01,1, length = 20)))
radsvm_model

### HPO plot
radsvm_hpo_plot <- ggplot(radsvm_model, aes(x = C, y = Accuracy)) +
  geom_line() +
  geom_point(fill = "white", colour = "black", size = 2) +
  labs(x = "Cost", y = "Accuracy") +
  ggtitle("Hyperparameter Optimization for Radial SVM Model") + 
  theme_few()

radsvm_model_class <- predict(radsvm_model, test_df_NT)
radsvm_model_prob <- predict(radsvm_model, test_df_NT, type = "prob")
conmat_radsvm_model <- confusionMatrix(data = radsvm_model_class, test_df_NT$Type)
conmat_radsvm_model

set.seed(2504)
radsvm_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "svmRadial", 
                               metric = "Accuracy",
                               trControl = cvspecs_grid,
                               )

radsvm_learnplot <- ggplot(radsvm_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("SVM Radial")


## SVM poly
set.seed(2504)
polysvm_model <- train(Type ~ solidity + extent + formfactor + compactness + aspect_ratio, 
                  data = train_df_NT, 
                  method = "svmPoly",
                  trControl = cvspecs_grid,
                  tuneGrid = expand.grid(degree = seq(1,5, length = 5), 
                                         scale = 0.1, 
                                         C = seq(1,10, length = 50)))
polysvm_model

### HPO plot
polysvm_hpo_plot <- ggplot(polysvm_model, aes(x = C, y = Accuracy)) +
  geom_line() +
  geom_point(fill = "white", colour = "black", size = 2) +
  labs(x = "Cost", y = "Accuracy") +
  ggtitle("Hyperparameter Optimization for Polynomial SVM Model") + 
  theme_few()

polysvm_model_class <- predict(polysvm_model, test_df_NT)
polysvm_model_prob <- predict(polysvm_model, test_df_NT, type = "prob")
conmat_polysvm_model <- confusionMatrix(data = polysvm_model_class, test_df_NT$Type)
conmat_polysvm_model

# learning curve
set.seed(2504)
polysvm_learning_curve <- learning_curve_dat(dat = train_df_NT %>% dplyr::select(c(3, 5:9)), 
                               outcome = "Type",
                               proportion = (1:10)/10, 
                               ## `train` arguments:
                               method = "svmPoly", 
                               metric = "Accuracy",
                               trControl = cvspecs_grid,
                               )

polysvm_learnplot <- ggplot(polysvm_learning_curve, aes(x = Training_Size, y = Accuracy, color = Data)) + 
  geom_smooth(method = loess, span = .8) + theme_few() + ggtitle("SVM Poly")

### Learning curves grid figure
learn_grid <- grid.arrange(mlr_learnplot,
             lda_learnplot,  
             qda_learnplot, 
             knn_learnplot,
             nb_learnplot,
             rf_learnplot,
             linsvm_learnplot,
             radsvm_learnplot,
             polysvm_learnplot,
             ncol = 2)

library(svglite)
ggsave("D:/PhD projects/Microfractures paper/learning_curves.svg", learn_grid, width = 12, height = 16, device = "svg")

### HPO plots grid figure
hpo_grid <- grid.arrange(knn_hpo_plot, 
             nb_hpo_plot, 
             rf_hpo_plot, 
             linsvm_hpo_plot, 
             radsvm_hpo_plot,
             polysvm_hpo_plot,
             ncol = 2)


```



This probability boxplot should be the one with the example shapes of the microfractures
and pores as it becomes intuitive. Where pores are >50% in being mistaken for microfracture is where we need to know the type of the pore.
```{r Fig ____ Probability boxplots for secondary labels}
library(reshape2)

## MLR
mlr_pred_class_df <- as.data.frame(mlr_model_class)
mlr_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = mlr_pred_class_df,
                       Crack_prob = mlr_model_prob[,1]
                       )
mlr_pred_type <- cbind(mlr_pred_type, 
                       misclassify = mlr_pred_type$Type != mlr_pred_type$mlr_model_class) %>% filter(Pore.type != "Unlabelled")

mlr_pred_misclassified <- mlr_pred_type %>% filter(misclassify == TRUE)

mlr_pred_type_melt <- melt(mlr_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
mlr_plot <- ggplot(mlr_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "Multiple logistic regression",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

mlr_plot

## LDA
lda_pred_class_df <- as.data.frame(lda_model_class)
lda_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = lda_pred_class_df,
                       Crack_prob = lda_model_prob[,1]
                       )
lda_pred_type <- cbind(lda_pred_type, 
                       misclassify = lda_pred_type$Type != lda_pred_type$lda_model_class) %>% filter(Pore.type != "Unlabelled")

lda_pred_misclassified <- lda_pred_type %>% filter(misclassify == TRUE)

lda_pred_type_melt <- melt(lda_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
lda_plot <- ggplot(lda_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "LDA",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

lda_plot

## QDA
qda_pred_class_df <- as.data.frame(qda_model_class)
qda_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = qda_pred_class_df,
                       Crack_prob = qda_model_prob[,1]
                       )
qda_pred_type <- cbind(qda_pred_type, 
                       misclassify = qda_pred_type$Type != qda_pred_type$qda_model_class) %>% filter(Pore.type != "Unlabelled")

qda_pred_type_melt <- melt(qda_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
qda_plot <- ggplot(qda_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "QDA",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

qda_plot

## kNN
knn_pred_class_df <- as.data.frame(knn_model_class)
knn_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = knn_pred_class_df,
                       Crack_prob = knn_model_prob[,1]
                       )
knn_pred_type <- cbind(knn_pred_type, 
                       misclassify = knn_pred_type$Type != knn_pred_type$knn_model_class) %>% filter(Pore.type != "Unlabelled")

knn_pred_type_melt <- melt(knn_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
knn_plot <- ggplot(knn_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "kNN",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

knn_plot

## nb
nb_pred_class_df <- as.data.frame(nb_model_class)
nb_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = nb_pred_class_df,
                       Crack_prob = nb_model_prob[,1]
                       )
nb_pred_type <- cbind(nb_pred_type, 
                       misclassify = nb_pred_type$Type != nb_pred_type$nb_model_class) %>% filter(Pore.type != "Unlabelled")

nb_pred_type_melt <- melt(nb_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
nb_plot <- ggplot(nb_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "Naive-Bayes",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

nb_plot

## rf
rf_pred_class_df <- as.data.frame(rf_model_class)
rf_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = rf_pred_class_df,
                       Crack_prob = rf_model_prob[,1]
                       )
rf_pred_type <- cbind(rf_pred_type, 
                       misclassify = rf_pred_type$Type != rf_pred_type$rf_model_class) %>% filter(Pore.type != "Unlabelled")

rf_pred_type_melt <- melt(rf_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
rf_plot <- ggplot(rf_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "Random Forest",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

rf_plot

## Linear SVM
linsvm_pred_class_df <- as.data.frame(linsvm_model_class)
linsvm_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = linsvm_pred_class_df,
                       Crack_prob = linsvm_model_prob[,1]
                       )
linsvm_pred_type <- cbind(linsvm_pred_type, 
                       misclassify = linsvm_pred_type$Type != linsvm_pred_type$linsvm_model_class) %>% filter(Pore.type != "Unlabelled")

linsvm_pred_type_melt <- melt(linsvm_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
linsvm_plot <- ggplot(linsvm_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "Linear SVM",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

linsvm_plot

## Radial SVM
radsvm_pred_class_df <- as.data.frame(radsvm_model_class)
radsvm_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = radsvm_pred_class_df,
                       Crack_prob = radsvm_model_prob[,1]
                       )
radsvm_pred_type <- cbind(radsvm_pred_type, 
                       misclassify = radsvm_pred_type$Type != radsvm_pred_type$radsvm_model_class) %>% filter(Pore.type != "Unlabelled")

radsvm_pred_type_melt <- melt(radsvm_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
radsvm_plot <- ggplot(radsvm_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "Radial SVM",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

radsvm_plot


## Poly SVM
polysvm_pred_class_df <- as.data.frame(polysvm_model_class)
polysvm_pred_type <- cbind(test_df_NT[,c(1:4,13)],
                       Predicted_class = polysvm_pred_class_df,
                       Crack_prob = polysvm_model_prob[,1]
                       )
polysvm_pred_type <- cbind(polysvm_pred_type, 
                       misclassify = polysvm_pred_type$Type != polysvm_pred_type$polysvm_model_class) %>% filter(Pore.type != "Unlabelled")

polysvm_pred_type_melt <- melt(polysvm_pred_type,id.vars='Pore.type', measure.vars='Crack_prob')
polysvm_plot <- ggplot(polysvm_pred_type_melt, aes(x=Pore.type, y=value, color=Pore.type)) +
  geom_boxplot() + 
  geom_jitter(color="grey", size=1.2, alpha=0.5) +
  labs(title = "Polynomial SVM",
       x = "Type of microfracture/pore",
       y = "Predicted probability of microfracture") +
  geom_hline(yintercept = 0.50, linetype = "dashed", linewidth = 1, color = "black") +
  theme_few() + theme(legend.position="none")

polysvm_plot


# Step 2: Arranging all the plots in a grid

sec_prob_grid <- grid.arrange(mlr_plot,
                              lda_plot,
                              qda_plot,
                              knn_plot,
                              nb_plot,
                              rf_plot,
                              linsvm_plot,
                              radsvm_plot,
                              polysvm_plot, nrow = 3, ncol = 3)



```


```{r Fig___ ROC curves and accuracy 95% confidence intervals, echo=FALSE}

library(MLeval)

roc_curve <- evalm(list(mlr_model,lda_model,qda_model,knn_model,nb_model,rf_model,
                   linsvm_model,radsvm_model, polysvm_model), 
              plots = c('r', 'cc'),
              cols = c("#e41a1c","#377eb8","#4daf4a","#984ea3","#ff7f00","#a65628","#f781bf","#873e23","#999999"),
              gnames=c('MLR',
                       'LDA',
                       'QDA',
                       'knn',
                       'NB',
                       'RF',
                       'Linear SVM',
                       'Radial SVM',
                       'Poly SVM'),
              rlinethick=0.8,
              fsize=12,
              showplots = TRUE)

roc_fig <- roc_curve$roc
cc_fig <- roc_curve$cc

ggsave("D:/PhD projects/Microfractures paper/images/Figure 9 - ROC curve + Acc boxplot/roc_curve.svg", roc_fig, width = 8, height = 8, device = "svg")

ggsave("D:/PhD projects/Microfractures paper/images/Figure 9 - ROC curve + Acc boxplot/cc_curve.svg", cc_fig, width = 8, height = 8, device = "svg")


resamps <- resamples(list(polySVM = polysvm_model,
                          radSVM = radsvm_model, 
                          linSVM = linsvm_model,
                          rf = rf_model,
                          knn = knn_model,
                          QDA = qda_model, 
                          LDA = lda_model, 
                          MLR = mlr_model,
                          NB = nb_model))


acc_boxplot <- bwplot(resamps)




```


```{r Fig___ Shap values, echo=FALSE}
## Shapley values using fastshap and shapviz
library(fastshap)
library(shapviz)
library(ggplot2)

pfun <- function(object, newdata){predict(object, newdata=newdata, type="prob")[,2]}
Y <- subset(train_df_NT, select = -c(1:4,10:13)) #only selecting the features
### MLR
set.seed(2504)
exp_mlr <- fastshap::explain(mlr_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

autoplot(exp_mlr)

# Visualizing using shapviz
shp_viz_mlr <- shapviz(exp_mlr, X_pred = data.matrix(Y), X = Y)

shap_mlr_plot <- sv_importance(shp_viz_mlr, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "MLR") +
  theme_minimal()
shap_mlr_plot

### Random forest
set.seed(2504)
exp_rf <- fastshap::explain(rf_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_rf)

# Visualizing using shapviz
shp_viz_rf <- shapviz(exp_rf, X_pred = data.matrix(Y), X = Y)

shap_rf_plot <- sv_importance(shp_viz_rf, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "Random Forest") +
  theme_minimal()
shap_rf_plot

### LDA
set.seed(2504)
exp_lda <- fastshap::explain(lda_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_lda)

# Visualizing using shapviz
shp_viz_lda <- shapviz(exp_lda, X_pred = data.matrix(Y), X = Y)

shap_lda_plot <- sv_importance(shp_viz_lda, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "LDA") +
  theme_minimal()

shap_lda_plot


### QDA
set.seed(2504)
exp_qda <- fastshap::explain(qda_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_qda)

# Visualizing using shapviz
shp_viz_qda <- shapviz(exp_qda, X_pred = data.matrix(Y), X = Y)

shap_qda_plot <- sv_importance(shp_viz_qda, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "QDA") +
  theme_minimal()
shap_qda_plot

### knn
set.seed(2504)
exp_knn <- fastshap::explain(knn_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_knn)

# Visualizing using shapviz
shp_viz_knn <- shapviz(exp_knn, X_pred = data.matrix(Y), X = Y)

shap_knn_plot <- sv_importance(shp_viz_knn, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "kNN") +
  theme_minimal()
shap_knn_plot

### NB
set.seed(2504)
exp_nb <- fastshap::explain(nb_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_nb)

# Visualizing using shapviz
shp_viz_nb <- shapviz(exp_nb, X_pred = data.matrix(Y), X = Y)

shap_nb_plot <- sv_importance(shp_viz_nb, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "Naive-Bayes") +
  theme_minimal()
shap_nb_plot

### Linear SVM
set.seed(2504)
exp_lsvm <- fastshap::explain(linsvm_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_lsvm)

# Visualizing using shapviz
shp_viz_lsvm <- shapviz(exp_lsvm, X_pred = data.matrix(Y), X = Y)

shap_lsvm_plot <- sv_importance(shp_viz_lsvm, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "Linear SVM") +
  theme_minimal()
shap_lsvm_plot

### Radial SVM
set.seed(2504)
exp_rsvm <- fastshap::explain(radsvm_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_rsvm)

# Visualizing using shapviz
shp_viz_rsvm <- shapviz(exp_rsvm, X_pred = data.matrix(Y), X = Y)

shap_rsvm_plot <- sv_importance(shp_viz_rsvm, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "Radial SVM") +
  theme_minimal()
shap_rsvm_plot

### Poly SVM
set.seed(2504)
exp_psvm <- fastshap::explain(polysvm_model, X = Y, nsim = 100, adjust = TRUE,  pred_wrapper = pfun)

#autoplot(exp_psvm)

# Visualizing using shapviz
shp_viz_psvm <- shapviz(exp_psvm, X_pred = data.matrix(Y), X = Y)

shap_psvm_plot <- sv_importance(shp_viz_psvm, kind = "both", alpha = 0.2, width = 0.2) +
  labs(title = "Polynomial SVM") +
  theme_minimal()
shap_psvm_plot


library(viridisLite)
my_colors <- colorRampPalette(c("#2D69A9", "#DE0018"))(256)

shap_psvm_plot2 <- sv_importance(shp_viz_psvm, 
                                 kind = "both", 
                                 alpha = 0.2, 
                                 width = 0.2,
                                 viridis_args = list(begin = 0.25, end = 0.85, option = "inferno")) +
  labs(title = "Polynomial SVM")  +
  theme_minimal()

shap_psvm_plot2

# Compiling all shap plots
shap_grid <- grid.arrange(shap_mlr_plot,
             shap_lda_plot, 
             shap_qda_plot, 
             shap_rf_plot, 
             shap_knn_plot,
             shap_nb_plot,
             shap_lsvm_plot,
             shap_rsvm_plot,
             shap_psvm_plot,
             nrow = 3, ncol = 3)

```
